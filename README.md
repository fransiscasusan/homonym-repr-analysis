# homonym-repr-analysis

Neural machine translation (NMT) has shown promising results in translating text or speech from one language to another and achieved state-of-the-art performances for various language pairs. However, little is known about what these models learn about language. 

In this project, we empirically evaluate the quality of NMT representations for learning the different meanings of homonyms through an extrinsic meaning classification task. We conduct a thorough investigation for both unidirectional and bidirectional NMT models trained with various target languages. We analyze how various target languages used in training NMT models affect how well NMT representations convey the different meanings of homonyms.
